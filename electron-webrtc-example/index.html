<!--
  ============================================================
  AIGuard Camera - Electron UI
   VERSION: 2.24.0 (2026-02-11)
   ============================================================
   
   CHANGELOG:
   - v2.24.0: ADD - Phase 6 (model-only, no mic) A/B isolation test for crash diagnosis
   - v2.13.0: CRITICAL FIX - Camera retry with backoff on NotReadableError (busy from clip recording)
              + Complete structural isolation: motion failure never blocks sound, sound failure never blocks motion
   - v2.12.0: DEBUG - 8-layer sound diagnostic system (CHECK1-CHECK8)
   - v2.11.0: FIX - Disabled noiseSuppression/echoCancellation for security audio
   - v2.9.0: ARCH - YAMNet loads via local HTTP server (main.js serves models)
   - v2.8.2: FIX - YAMNet model URL fallback chain (GCS direct ‚Üí Kaggle)
   - v2.8.1: FIX - YAMNet model URL updated from TFHub (403) to Kaggle CDN
   - v2.8.0: CRITICAL - Inline YAMNet sound detection integration (was placeholder only)
  - v2.7.0: Added sensor status indicator (camera/mic icons) in success screen
  - v2.6.0: CRITICAL FIX - Camera/Sound separation: camera only activates when motion is enabled
  - v2.5.2: Changed debounce to 60s default for security best practice
  - v2.3.2: CRITICAL FIX - Camera initialization with progressive fallbacks + clean-start sequence
  - v2.3.1: Snapshot validation for readyState/dimensions
  - v2.3.0: Added inline MediaPipe motion detection with event reporting
  - v2.2.1: Fixed monitoring initialization - proper IPC listener setup for camera start ACK
  - v2.2.0: Added renderer-monitoring.js script for motion/sound detection
  - v2.1.0: Disabled camera preflight check for Away Mode (LED fix)
  - v2.0.0: Full monitoring integration with motion/sound detection
  
  DEPENDENCIES: Electron preload.js, renderer-webrtc.js, @mediapipe/tasks-vision (CDN)
  ============================================================
-->
<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AIGuard Camera</title>
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
        min-height: 100vh;
        display: flex;
        justify-content: center;
        align-items: center;
        color: #e2e8f0;
      }

      .container {
        background: rgba(30, 41, 59, 0.9);
        backdrop-filter: blur(10px);
        border-radius: 16px;
        padding: 40px;
        width: 420px;
        box-shadow: 0 25px 50px -12px rgba(0, 0, 0, 0.5);
        border: 1px solid rgba(71, 85, 105, 0.5);
        position: relative;
      }

      .header {
        text-align: center;
        margin-bottom: 28px;
      }

      .logo {
        font-size: 48px;
        margin-bottom: 12px;
      }

      h1 {
        font-size: 24px;
        font-weight: 700;
        margin-bottom: 8px;
        color: #f1f5f9;
      }

      .subtitle {
        color: #94a3b8;
        font-size: 14px;
        line-height: 1.35;
      }

      .lang-toggle {
        position: absolute;
        top: 18px;
        right: 18px;
        display: flex;
        gap: 8px;
      }

      .lang-btn {
        padding: 8px 14px;
        border: 1px solid rgba(71, 85, 105, 0.5);
        background: rgba(30, 41, 59, 0.8);
        color: #94a3b8;
        border-radius: 10px;
        cursor: pointer;
        font-size: 13px;
        transition: all 0.2s;
        user-select: none;
      }

      .lang-btn:hover {
        background: rgba(51, 65, 85, 0.8);
        color: #e2e8f0;
      }

      .lang-btn.active {
        background: #3b82f6;
        border-color: #3b82f6;
        color: #ffffff;
      }

      .code-section {
        margin-bottom: 18px;
      }

      .code-label {
        display: block;
        margin-bottom: 12px;
        font-size: 13px;
        color: #cbd5e1;
        text-align: center;
      }

      .code-inputs {
        display: flex;
        gap: 8px;
        justify-content: center;
        margin-bottom: 14px;
      }

      .code-input {
        width: 50px;
        height: 58px;
        text-align: center;
        font-size: 24px;
        font-weight: 700;
        border: 2px solid rgba(71, 85, 105, 0.5);
        border-radius: 12px;
        background: rgba(15, 23, 42, 0.6);
        color: #f1f5f9;
        outline: none;
        transition: all 0.2s;
      }

      .code-input:focus {
        border-color: #3b82f6;
        box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.3);
      }

      .paste-btn {
        width: 100%;
        padding: 12px;
        background: rgba(51, 65, 85, 0.6);
        border: 1px solid rgba(71, 85, 105, 0.5);
        color: #94a3b8;
        border-radius: 10px;
        cursor: pointer;
        font-size: 14px;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 8px;
        transition: all 0.2s;
        user-select: none;
      }

      .paste-btn:hover {
        background: rgba(71, 85, 105, 0.6);
        color: #e2e8f0;
      }

      .submit-btn,
      .minimize-btn {
        width: 100%;
        padding: 14px;
        background: linear-gradient(135deg, #3b82f6 0%, #2563eb 100%);
        border: none;
        color: white;
        border-radius: 12px;
        cursor: pointer;
        font-size: 16px;
        font-weight: 700;
        transition: all 0.2s;
        margin-top: 14px;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 10px;
        user-select: none;
      }

      .submit-btn:hover:not(:disabled),
      .minimize-btn:hover {
        transform: translateY(-2px);
        box-shadow: 0 12px 22px rgba(59, 130, 246, 0.28);
      }

      .submit-btn:disabled {
        opacity: 0.5;
        cursor: not-allowed;
        transform: none;
        box-shadow: none;
      }

      .error-message {
        background: rgba(239, 68, 68, 0.18);
        border: 1px solid rgba(239, 68, 68, 0.45);
        color: #fecaca;
        padding: 12px;
        border-radius: 10px;
        text-align: center;
        margin-top: 14px;
        font-size: 14px;
        display: none;
        line-height: 1.35;
      }

      .success-screen {
        display: none;
        text-align: center;
      }

      .success-icon {
        font-size: 64px;
        margin-bottom: 12px;
      }

      .success-title {
        font-size: 24px;
        font-weight: 800;
        color: #4ade80;
        margin-bottom: 8px;
      }

      .success-subtitle {
        color: #94a3b8;
        margin-bottom: 18px;
        line-height: 1.35;
      }

      .info-box {
        background: rgba(59, 130, 246, 0.1);
        border: 1px solid rgba(59, 130, 246, 0.28);
        border-radius: 14px;
        padding: 16px;
        margin-bottom: 16px;
        text-align: start;
      }

      .info-box-title {
        font-weight: 800;
        color: #93c5fd;
        margin-bottom: 10px;
        display: flex;
        align-items: center;
        gap: 8px;
        font-size: 14px;
      }

      .info-box-text {
        color: #cbd5e1;
        font-size: 13px;
        line-height: 1.55;
      }

      .action-buttons {
        display: flex;
        flex-direction: column;
        gap: 12px;
      }

      .close-btn {
        width: 100%;
        padding: 14px;
        background: rgba(239, 68, 68, 0.18);
        border: 1px solid rgba(239, 68, 68, 0.45);
        color: #fecaca;
        border-radius: 12px;
        cursor: pointer;
        font-size: 16px;
        font-weight: 800;
        transition: all 0.2s;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 10px;
        user-select: none;
      }

      .close-btn:hover {
        background: rgba(239, 68, 68, 0.26);
      }

      .warning-text {
        color: #fbbf24;
        font-size: 12px;
        margin-top: 6px;
        text-align: center;
        line-height: 1.35;
      }

      .hint-text {
        color: #94a3b8;
        font-size: 12px;
        margin-top: 10px;
        text-align: center;
        line-height: 1.35;
      }

      .spinner {
        width: 20px;
        height: 20px;
        border: 2px solid rgba(255, 255, 255, 0.35);
        border-radius: 50%;
        border-top-color: white;
        animation: spin 1s linear infinite;
      }

      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }

      .live-indicator {
        display: none;
        background: rgba(239, 68, 68, 0.2);
        border: 1px solid rgba(239, 68, 68, 0.5);
        border-radius: 10px;
        padding: 12px;
        margin-bottom: 16px;
        text-align: center;
      }

      .live-indicator.active {
        display: block;
      }

      .live-dot {
        display: inline-block;
        width: 10px;
        height: 10px;
        background: #ef4444;
        border-radius: 50%;
        margin-right: 8px;
        animation: pulse 1.5s infinite;
      }

      @keyframes pulse {
        0%,
        100% {
          opacity: 1;
        }
        50% {
          opacity: 0.5;
        }
      }

      .live-text {
        color: #fecaca;
        font-weight: 700;
      }

      [dir="rtl"] .lang-toggle {
        right: auto;
        left: 18px;
      }

      [dir="rtl"] .info-box {
        text-align: right;
      }

      [dir="rtl"] .code-inputs {
        flex-direction: row-reverse;
      }

      [dir="rtl"] .info-box-title {
        flex-direction: row-reverse;
      }

      [dir="rtl"] .live-dot {
        margin-right: 0;
        margin-left: 8px;
      }

      /* Sensor status indicator */
      .sensor-status {
        display: none;
        justify-content: center;
        gap: 16px;
        padding: 10px 14px;
        border-radius: 10px;
        background: rgba(30, 41, 59, 0.7);
        border: 1px solid rgba(71, 85, 105, 0.4);
        margin-bottom: 16px;
      }

      .sensor-status.active {
        display: flex;
      }

      .sensor-item {
        display: flex;
        align-items: center;
        gap: 6px;
        font-size: 13px;
        font-weight: 600;
      }

      .sensor-item.on {
        color: #4ade80;
      }

      .sensor-item.off {
        color: #64748b;
      }

      .sensor-dot {
        width: 6px;
        height: 6px;
        border-radius: 50%;
        background: #4ade80;
        animation: pulse 1.5s infinite;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="lang-toggle">
        <button id="lang-en" class="lang-btn active" type="button">EN</button>
        <button id="lang-he" class="lang-btn" type="button">◊¢◊ë</button>
      </div>

      <div id="pairing-screen">
        <div class="header">
          <div class="logo">üõ°Ô∏è</div>
          <h1 id="title">Connect Camera</h1>
          <div class="subtitle" id="subtitle">Enter the 6-digit code from your dashboard</div>
        </div>

        <div class="code-section">
          <label class="code-label" id="code-label">Pairing Code</label>
          <div class="code-inputs" id="code-inputs">
            <input class="code-input" inputmode="numeric" maxlength="1" />
            <input class="code-input" inputmode="numeric" maxlength="1" />
            <input class="code-input" inputmode="numeric" maxlength="1" />
            <input class="code-input" inputmode="numeric" maxlength="1" />
            <input class="code-input" inputmode="numeric" maxlength="1" />
            <input class="code-input" inputmode="numeric" maxlength="1" />
          </div>

          <button class="paste-btn" id="paste-btn" type="button">üìã <span id="paste-text">Paste Code</span></button>
          <div class="hint-text" id="paste-hint">Tip: You can paste the full code and we will fill it automatically.</div>
        </div>

        <button class="submit-btn" id="connect-btn" type="button">Connect</button>
        <div class="error-message" id="error"></div>
      </div>

      <div id="success-screen" class="success-screen">
        <div class="success-icon">‚úÖ</div>
        <div class="success-title" id="success-title">Connected Successfully</div>
        <div class="success-subtitle" id="success-subtitle">Your camera is now linked to your account</div>

        <!-- Sensor Status Indicator (camera/mic) -->
        <div id="sensor-status" class="sensor-status">
          <div class="sensor-item off" id="sensor-camera">
            üì∑ <span id="sensor-camera-label">Camera</span>
          </div>
          <div class="sensor-item off" id="sensor-mic">
            üé§ <span id="sensor-mic-label">Mic</span>
          </div>
        </div>

        <!-- Away Mode Status -->
        <div id="away-mode-indicator" class="info-box" style="display: none; background: rgba(34, 197, 94, 0.15); border-color: rgba(34, 197, 94, 0.4);">
          <div class="info-box-title" style="color: #86efac;"><span>üè†</span> <span id="away-mode-title">Away Mode Active</span></div>
          <div class="info-box-text" id="away-mode-text">Camera is monitoring. Display will turn off.</div>
        </div>

        <div id="live-indicator" class="live-indicator">
          <span class="live-dot"></span>
          <span class="live-text" id="live-text">LIVE - Streaming</span>
        </div>

        <div class="info-box">
          <div class="info-box-title"><span>‚ÑπÔ∏è</span> <span id="bg-title">Background Operation</span></div>
          <div class="info-box-text" id="bg-text">
            The camera will continue to work in the background even when this window is closed or minimized.
            To reopen the app, click the AIGuard icon near the clock.
          </div>
        </div>

        <div class="action-buttons">
          <button class="minimize-btn" id="minimize-btn" type="button">üîΩ <span id="minimize-text">Minimize to Background</span></button>
          <div>
            <button class="close-btn" id="exit-btn" type="button">‚õî <span id="exit-text">Exit Application</span></button>
            <div class="warning-text" id="exit-warning">Warning: Exiting will stop the camera service</div>
          </div>
        </div>
      </div>

      <!-- User Returned Modal removed - Away Mode is controlled manually from Dashboard -->
    </div>

    <!-- BUILD ID (debug)
         If you don't see this in the Electron console, you're not running
         this updated index.html file (or DevTools console is filtered). -->
    <script>
      console.log("[UI] index.html build: electron-index-2026-02-11-v2.24.0");
      
      // ============================================================
      // Sensor Status Indicator - update UI for camera/mic
      // ============================================================
      function updateSensorIndicator({ motion, sound }) {
        const container = document.getElementById('sensor-status');
        const cameraEl = document.getElementById('sensor-camera');
        const micEl = document.getElementById('sensor-mic');
        if (!container || !cameraEl || !micEl) return;

        const isActive = motion || sound;
        container.classList.toggle('active', isActive);

        cameraEl.className = 'sensor-item ' + (motion ? 'on' : 'off');
        cameraEl.innerHTML = (motion ? 'üì∑ ' : 'üì∑ ') + '<span>' + (motion ? (document.documentElement.dir === 'rtl' ? '◊û◊¶◊ú◊û◊î ‚úì' : 'Camera ‚úì') : (document.documentElement.dir === 'rtl' ? '◊û◊¶◊ú◊û◊î' : 'Camera')) + '</span>' + (motion ? ' <span class="sensor-dot"></span>' : '');

        micEl.className = 'sensor-item ' + (sound ? 'on' : 'off');
        micEl.innerHTML = (sound ? 'üé§ ' : 'üé§ ') + '<span>' + (sound ? (document.documentElement.dir === 'rtl' ? '◊û◊ô◊ß◊®◊ï◊§◊ï◊ü ‚úì' : 'Mic ‚úì') : (document.documentElement.dir === 'rtl' ? '◊û◊ô◊ß◊®◊ï◊§◊ï◊ü' : 'Mic')) + '</span>' + (sound ? ' <span class="sensor-dot"></span>' : '');
      }
    </script>

    <!-- WebRTC logic (start/stop via IPC) -->
    <script src="renderer-webrtc.js"></script>

    <script>
      // ============================================================
      // v2.5.0: RENDERER GUARDRAILS - catch errors before they crash
      // ============================================================
      window.addEventListener('error', (event) => {
        console.error('[GUARDRAIL] Uncaught error:', event.message, 'at', event.filename + ':' + event.lineno);
        try { window.electronAPI?.reportRendererError?.('error', event.message, event.filename, event.lineno); } catch (_) {}
      });
      window.addEventListener('unhandledrejection', (event) => {
        const reason = event.reason?.message || event.reason || 'unknown';
        console.error('[GUARDRAIL] Unhandled promise rejection:', reason);
        try { window.electronAPI?.reportRendererError?.('rejection', String(reason)); } catch (_) {}
      });
      
      // ============================================================
      // BUILD ID (debug)
      // If you don't see this line in the Electron console, you're not
      // running the updated index.html file.
      // ============================================================
      const __ELECTRON_INDEX_BUILD_ID__ = "electron-index-2026-02-11-v2.24.0";
      console.log(`[UI] index.html build: ${__ELECTRON_INDEX_BUILD_ID__}`);
      
      // ============================================================
      // MONITORING SYSTEM - Inline MediaPipe Integration
      // VERSION: 2.5.0 - Startup grace period + isMonitoring guard
      // ============================================================
      (async function initMonitoringSystem() {
        console.log('[Monitoring] Initializing monitoring system v2.5.2...');
        
        // Supabase config (for event reporting)
        const SUPABASE_URL = 'https://zoripeohnedivxkvrpbi.supabase.co';
        const EVENTS_REPORT_ENDPOINT = `${SUPABASE_URL}/functions/v1/events-report`;
        
        // State
        let monitoringStream = null;
        let videoElement = null;
        let objectDetector = null;
        let isMonitoring = false;
        let isDetectorReady = false;
        let detectionLoopActive = false;
        let currentConfig = null;
        let deviceAuthToken = null;
        let deviceId = null;
        
        // Debouncing
        const lastDetectionTime = {};
        const DEBOUNCE_MS = 60000; // 60 seconds - prevent rapid firing
        let isProcessingEvent = false; // Block concurrent sends
        
        // CRITICAL: Startup grace period - no events for first 10 seconds
        const STARTUP_GRACE_PERIOD_MS = 10000;
        let monitoringStartedAt = null;
        
        // ============================================================
        // SOUND DETECTION STATE (YAMNet) - Single Mode v3.0.0
        // ============================================================
        let soundModel = null;
        let soundAudioContext = null;
        let soundMediaStream = null;
        let soundProcessor = null;
        let soundSource = null;
        let soundSilentGain = null; // v2.22.0 Bonus B: GainNode(0) to prevent audio feedback
        let isSoundRunning = false;
        let isSoundModelReady = false;
        let _isSoundModelLoading = false;
        let _inferenceInFlight = false; // v2.22.0 Fix 1: Single-flight inference lock
        let _inferenceDropCount = 0;    // v2.22.0 Fix 1: Track dropped frames
        
        // v2.22.0 Bonus A: Ring buffer instead of push(...chunk) ‚Äî avoids GC pressure
        const SOUND_RING_BUFFER_SIZE = 32000; // ~2 seconds at 16kHz
        let soundRingBuffer = new Float32Array(SOUND_RING_BUFFER_SIZE);
        let soundRingWriteIdx = 0;
        let soundRingAvailable = 0;
        
        const SOUND_SAMPLE_RATE = 16000;
        const SOUND_FRAME_LENGTH = 0.975;
        const SOUND_SAMPLES_NEEDED = Math.floor(SOUND_SAMPLE_RATE * SOUND_FRAME_LENGTH);
        const SOUND_RMS_THRESHOLD = 0.001;
        let soundLastDetectionTime = {};
        let soundPersistenceCount = {};
        let soundScoreHistory = {};
        let isProcessingSoundEvent = false;
        let audioProcessCount = 0;
        let inferenceCount = 0;
        let lastAudioProcessTs = 0;
        let _bufferReachedLogged = false;
        let _soundAudioDiagInterval = null;
        let _rmsSkipLogTimer = 0;
        
        // v2.21.0: Diagnostic isolation phases
        const SOUND_MIC_TO_INFERENCE_DELAY_MS = 800;
        
        // ‚ïê‚ïê‚ïê SINGLE MODE: Only one sound mode active at a time ‚ïê‚ïê‚ïê
        let currentSoundMode = null; // 'baby_cry' | 'dog_bark' | 'help'
        let currentSoundTargetLabel = null; // YAMNet label: 'baby_crying' | 'dog_barking' | 'scream'
        
        // MVP thresholds (intentionally low to prove E2E, tune later)
        const SOUND_MODE_CONFIG = {
          baby_cry: { yamnet_label: 'baby_crying', threshold: 0.12, persistence: 1, debounce_ms: 30000 },
          dog_bark: { yamnet_label: 'dog_barking', threshold: 0.15, persistence: 1, debounce_ms: 60000 },
          help:     { yamnet_label: 'scream',      threshold: 0.15, persistence: 2, debounce_ms: 15000 },
        };
        
        // YAMNet target class indices (only 3 active sound categories)
        const YAMNET_TARGET_CLASSES = {
          22: { label: 'baby_crying', name: 'Crying, sobbing' },
          23: { label: 'baby_crying', name: 'Baby cry, infant cry' },
          24: { label: 'baby_crying', name: 'Whimper' },
          67: { label: 'dog_barking', name: 'Bark' },
          68: { label: 'dog_barking', name: 'Yip' },
          69: { label: 'dog_barking', name: 'Howl' },
          70: { label: 'dog_barking', name: 'Growling' },
          20: { label: 'scream', name: 'Screaming' },
          21: { label: 'scream', name: 'Wail, moan' },
          19: { label: 'scream', name: 'Shout' },
        };
        
        // Label mapping from COCO to our categories
        const LABEL_MAPPING = {
          person: 'person',
          cat: 'animal', dog: 'animal', bird: 'animal', horse: 'animal',
          sheep: 'animal', cow: 'animal', elephant: 'animal', bear: 'animal',
          car: 'vehicle', truck: 'vehicle', bus: 'vehicle', motorcycle: 'vehicle',
          bicycle: 'vehicle', airplane: 'vehicle', boat: 'vehicle',
        };
        
        // ============================================================
        // Load MediaPipe Tasks Vision from CDN
        // ============================================================
        async function loadMediaPipe() {
          try {
            console.log('[Monitoring] Loading MediaPipe Tasks Vision from CDN...');
            
            // Import the library dynamically
            const vision = await import('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/vision_bundle.mjs');
            const { ObjectDetector, FilesetResolver } = vision;
            
            console.log('[Monitoring] ‚úì MediaPipe library loaded');
            
            // Load WASM files
            const wasmFileset = await FilesetResolver.forVisionTasks(
              'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.14/wasm'
            );
            
            // Create object detector
            objectDetector = await ObjectDetector.createFromOptions(wasmFileset, {
              baseOptions: {
                modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/float16/1/efficientdet_lite0.tflite',
                delegate: 'CPU', // v2.19.0: Changed from GPU to CPU to prevent ACCESS_VIOLATION crashes
              },
              runningMode: 'VIDEO',
              maxResults: 5,
              scoreThreshold: 0.5,
            });
            
            isDetectorReady = true;
            console.log('[Monitoring] ‚úì Object detector initialized');
            return true;
          } catch (error) {
            console.error('[Monitoring] ‚úó MediaPipe initialization failed:', error);
            return false;
          }
        }
        
        // ============================================================
        // Capture snapshot as base64
        // ============================================================
        function captureSnapshot() {
          if (!videoElement || videoElement.readyState < 2) return null;
          try {
            const canvas = document.createElement('canvas');
            canvas.width = videoElement.videoWidth || 640;
            canvas.height = videoElement.videoHeight || 480;
            const ctx = canvas.getContext('2d');
            ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            return canvas.toDataURL('image/jpeg', 0.8);
          } catch (e) {
            console.error('[Monitoring] Snapshot failed:', e);
            return null;
          }
        }
        
        // ============================================================
        // Send event to server
        // ============================================================
        async function sendEventToServer(eventData) {
          if (!deviceAuthToken || !deviceId) {
            console.warn('[Monitoring] Missing deviceAuthToken or deviceId, skipping event');
            return;
          }
          
          // CRITICAL: Check if monitoring is actually active
          if (!isMonitoring) {
            console.log('[Monitoring] Monitoring not active, skipping event');
            return;
          }
          
          // CRITICAL: Startup grace period - skip events in first 10 seconds
          if (monitoringStartedAt && (Date.now() - monitoringStartedAt) < STARTUP_GRACE_PERIOD_MS) {
            const elapsed = Date.now() - monitoringStartedAt;
            console.log(`[Monitoring] Startup grace period (${elapsed}ms/${STARTUP_GRACE_PERIOD_MS}ms) - skipping event`);
            return;
          }
          
          // Prevent concurrent sends - only one event at a time
          if (isProcessingEvent) {
            console.log('[Monitoring] Already processing an event, skipping');
            return;
          }
          
          try {
            isProcessingEvent = true;
            console.log(`[Monitoring] Sending event: ${eventData.label} (${(eventData.confidence * 100).toFixed(0)}%)`);
            
            const snapshot = captureSnapshot();
            
            // CRITICAL: Only send if we have a snapshot
            if (!snapshot) {
              console.log('[Monitoring] No snapshot available, skipping event');
              isProcessingEvent = false;
              return;
            }
            
            const response = await fetch(EVENTS_REPORT_ENDPOINT, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'x-device-token': deviceAuthToken,
              },
              body: JSON.stringify({
                device_id: deviceId,
                event_type: 'motion',
                labels: [{ label: eventData.label, confidence: eventData.confidence }],
                snapshot: snapshot,
                timestamp: Date.now(),
                metadata: eventData.metadata || {},
              }),
            });
            
            if (!response.ok) {
              const errorText = await response.text();
              console.error('[Monitoring] Event report failed:', response.status, errorText);
            } else {
              const result = await response.json();
              console.log('[Monitoring] ‚úì Event reported:', result.ai_is_real ? 'REAL' : 'False Positive');
            }
          } catch (error) {
            console.error('[Monitoring] Event send error:', error);
          } finally {
            isProcessingEvent = false;
          }
        }
        
        // ============================================================
        // SOUND: Send sound event to server (no snapshot required)
        // ============================================================
        async function sendSoundEventToServer(eventData) {
          if (!deviceAuthToken || !deviceId) {
            console.warn('[Monitoring] Missing deviceAuthToken or deviceId, skipping sound event');
            return;
          }
          if (!isMonitoring) {
            console.log('[Monitoring] Monitoring not active, skipping sound event');
            return;
          }
          if (monitoringStartedAt && (Date.now() - monitoringStartedAt) < STARTUP_GRACE_PERIOD_MS) {
            console.log(`[Monitoring] Startup grace period - skipping sound event`);
            return;
          }
          if (isProcessingSoundEvent) {
            console.log('[Monitoring] Already processing a sound event, skipping');
            return;
          }
          
          try {
            isProcessingSoundEvent = true;
            
            // Step 6: Explicit SENDING log before fetch
            console.log(`[Sound] SENDING SOUND EVENT - event_type=sound, sound_mode=${currentSoundMode}, label=${currentSoundTargetLabel}, confidence=${(eventData.confidence * 100).toFixed(1)}%`);
            
            const response = await fetch(EVENTS_REPORT_ENDPOINT, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'x-device-token': deviceAuthToken,
              },
              body: JSON.stringify({
                device_id: deviceId,
                event_type: 'sound',
                labels: [{ label: currentSoundMode, confidence: eventData.confidence }],
                snapshot: null,
                timestamp: Date.now(),
                metadata: {
                  sound_mode: currentSoundMode,
                  yamnet_index: eventData.metadata?.yamnet_index,
                  yamnet_score: eventData.confidence,
                  yamnet_class: eventData.metadata?.yamnet_class,
                  ...(eventData.metadata || {}),
                },
              }),
            });
            
            // Step 6: Log HTTP response status + body
            if (!response.ok) {
              const errorText = await response.text();
              console.error(`[Sound] events-report FAILED: HTTP ${response.status} ‚Äî ${errorText}`);
            } else {
              const result = await response.json();
              console.log(`[Sound] events-report OK: HTTP ${response.status} ‚Äî ${JSON.stringify({
                event_id: result.event_id,
                ai_is_real: result.ai_is_real,
                ai_confidence: result.ai_confidence,
                notification_sent: result.notification_sent,
              })}`);
            }
          } catch (error) {
            console.error('[Monitoring] Sound event send error:', error);
          } finally {
            isProcessingSoundEvent = false;
          }
        }
        
        // ============================================================
        // SOUND: Load TensorFlow.js + YAMNet
        // ============================================================
        async function loadYAMNet() {
          // v2.21.0 Step 4: Prevent concurrent model loads
          if (_isSoundModelLoading) {
            console.warn('[Sound] ‚ö†Ô∏è Model already loading ‚Äî skipping duplicate loadYAMNet() call');
            // Wait for the existing load to finish
            while (_isSoundModelLoading) {
              await new Promise(r => setTimeout(r, 200));
            }
            return isSoundModelReady;
          }
          if (isSoundModelReady && soundModel) {
            console.log('[Sound] Model already loaded ‚Äî reusing');
            return true;
          }
          _isSoundModelLoading = true;
          const loadStartMs = performance.now();
          try {
            console.log('[Sound] Loading TensorFlow.js (CPU-only, WebGL DISABLED)...');
            
            if (typeof tf === 'undefined') {
              // v2.19.0: Load TF.js with COMPLETE WebGL prevention
              // Step 1: Load core FIRST
              await new Promise((resolve, reject) => {
                const script = document.createElement('script');
                script.src = 'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-core@4.17.0/dist/tf-core.min.js';
                script.onload = resolve;
                script.onerror = reject;
                document.head.appendChild(script);
              });
              console.log('[Sound] ‚úì tfjs-core loaded');
              
              // Step 2: IMMEDIATELY disable ALL WebGL probing BEFORE any other TF.js module loads
              // This prevents native GPU access that causes ACCESS_VIOLATION (0xC0000005)
              try {
                tf.env().set('WEBGL_VERSION', 0);
                tf.env().set('HAS_WEBGL', false);
                tf.env().set('WEBGL_CPU_FORWARD', false);
                tf.env().set('WEBGL_PACK', false);
                tf.env().set('WEBGL_RENDER_FLOAT32_CAPABLE', false);
                tf.env().set('WEBGL_FLUSH_THRESHOLD', -1);
                console.log('[Sound] ‚úì WebGL env flags DISABLED (crash prevention)');
              } catch (envErr) {
                console.warn('[Sound] ‚ö†Ô∏è Could not set all WebGL env flags:', envErr.message);
              }
              
              // Step 3: Now load CPU backend and converter (safe - no GPU probing)
              const remainingScripts = [
                'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-backend-cpu@4.17.0/dist/tf-backend-cpu.min.js',
                'https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-converter@4.17.0/dist/tf-converter.min.js'
              ];
              for (const src of remainingScripts) {
                await new Promise((resolve, reject) => {
                  const script = document.createElement('script');
                  script.src = src;
                  script.onload = resolve;
                  script.onerror = reject;
                  document.head.appendChild(script);
                });
              }
              console.log('[Sound] ‚úì TensorFlow.js (CPU-only, WebGL blocked) loaded');
            } else {
              // TF already loaded - ensure WebGL is still disabled
              try {
                tf.env().set('WEBGL_VERSION', 0);
                tf.env().set('HAS_WEBGL', false);
              } catch (_) {}
            }
            
            // v2.22.0 Fix 2: Explicit backend verification ‚Äî FORCE cpu, never webgl
            const backendBefore = typeof tf !== 'undefined' ? tf.getBackend() : 'N/A';
            console.log(`[Sound] TF.js backend BEFORE setBackend: "${backendBefore}"`);
            await tf.setBackend('cpu');
            await tf.ready();
            const backendAfter = tf.getBackend();
            console.log(`[Sound] TF.js backend AFTER setBackend('cpu'): "${backendAfter}"`);
            if (backendAfter !== 'cpu') {
              console.error(`[Sound] ‚úó CRITICAL: Backend is "${backendAfter}" instead of "cpu"! Aborting to prevent crash.`);
              _isSoundModelLoading = false;
              return false;
            }
            
            console.log('[Sound] Loading YAMNet model...');
            // Load from local HTTP server (main.js serves monitoring/models/)
            const port = await window.electronAPI?.getModelServerPort?.();
            if (!port) {
              throw new Error('Local model server not running. Ensure YAMNet files exist in monitoring/models/yamnet/');
            }
            const localModelUrl = `http://127.0.0.1:${port}/yamnet/model.json`;
            console.log(`[Sound] Loading from: ${localModelUrl}`);
            soundModel = await tf.loadGraphModel(localModelUrl);
            
            // ‚ïê‚ïê‚ïê CHECK 1: Model loaded confirmation with timing ‚ïê‚ïê‚ïê
            const loadDurationMs = Math.round(performance.now() - loadStartMs);
            console.log(`[Sound] ‚úÖ CHECK1: YAMNet model loaded OK in ${loadDurationMs}ms from ${localModelUrl}`);
            
            const dummy = tf.zeros([15600]);
            const warmup = soundModel.predict(dummy);
            // warmup may be a single tensor or an array of tensors
            if (Array.isArray(warmup)) {
              warmup.forEach(t => t.dispose());
            } else {
              warmup.dispose();
            }
            dummy.dispose();
            
            isSoundModelReady = true;
            _isSoundModelLoading = false;
            console.log('[Sound] ‚úì YAMNet model loaded and warmed up');
            return true;
          } catch (error) {
            const loadDurationMs = Math.round(performance.now() - loadStartMs);
            console.error(`[Sound] ‚úó CHECK1 FAIL: YAMNet initialization failed after ${loadDurationMs}ms:`, error);
            console.error(`[Sound] ‚úó Model URL attempted: ${error._modelUrl || 'unknown'}`);
            isSoundModelReady = false;
            _isSoundModelLoading = false;
            return false;
          }
        }
        
        // ============================================================
        // SOUND: Start microphone capture + inference
        // v2.21.0: 5-step crash isolation system
        // ============================================================
        async function startSoundDetection(config) {
          // ‚ïê‚ïê‚ïê STEP 4: Absolute double-start prevention ‚ïê‚ïê‚ïê
          if (isSoundRunning) {
            console.warn('[Sound] ‚ö†Ô∏è DOUBLE-START BLOCKED ‚Äî isSoundRunning=true, ignoring duplicate start');
            return true;
          }
          
          // v2.21.0: Force-cleanup any leftover resources before starting
          // This prevents orphaned AudioContexts / streams from previous crashed sessions
          if (soundAudioContext || soundMediaStream || soundProcessor) {
            console.warn('[Sound] ‚ö†Ô∏è Stale resources detected ‚Äî cleaning up before start');
            await stopSoundDetection();
          }
          
          const debugPhase = config?.sensors?.sound?.debug_phase || 0;
          console.log(`[Sound] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
          console.log(`[Sound] startSoundDetection v2.21.0 ‚Äî debug_phase=${debugPhase}`);
          console.log(`[Sound] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
          
          // ‚ïê‚ïê‚ïê STEP 1 / STEP 5: Mic-only isolation (no TFJS) ‚ïê‚ïê‚ïê
          if (debugPhase === 1 || debugPhase === 5) {
            console.log(`[Sound] PHASE ${debugPhase}: Mic-only test (NO TFJS, NO model)`);
            try {
              // v2.23.0: Minimal constraints ‚Äî avoid sampleRate/processing flags that cause driver crashes on Windows
              soundMediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
              const micTrack = soundMediaStream.getAudioTracks()[0];
              console.log(`[Sound] PHASE ${debugPhase}: ‚úì Mic opened: ${micTrack?.label || 'unknown'}`);
              
              soundAudioContext = new AudioContext({ sampleRate: SOUND_SAMPLE_RATE });
              soundSource = soundAudioContext.createMediaStreamSource(soundMediaStream);
              soundProcessor = soundAudioContext.createScriptProcessor(4096, 1, 1);
              
              let rmsCount = 0;
              soundProcessor.onaudioprocess = (event) => {
                rmsCount++;
                const data = event.inputBuffer.getChannelData(0);
                const rms = Math.sqrt(data.reduce((sum, s) => sum + s * s, 0) / data.length);
                if (rmsCount <= 5 || rmsCount % 20 === 0) {
                  console.log(`[Sound] PHASE ${debugPhase}: RMS #${rmsCount} = ${rms.toFixed(6)}`);
                }
              };
              
              soundSource.connect(soundProcessor);
              // v2.22.0 Bonus B: Connect via silent GainNode to prevent audio feedback
              soundSilentGain = soundAudioContext.createGain();
              soundSilentGain.gain.value = 0;
              soundProcessor.connect(soundSilentGain);
              soundSilentGain.connect(soundAudioContext.destination);
              await soundAudioContext.resume();
              
              isSoundRunning = true;
              console.log(`[Sound] PHASE ${debugPhase}: ‚úì Audio pipeline running ‚Äî will auto-stop in 20s`);
              
              // Auto-stop after 20 seconds for diagnostic
              setTimeout(() => {
                if (isSoundRunning) {
                  console.log(`[Sound] PHASE ${debugPhase}: 20s elapsed ‚Äî auto-stopping diagnostic`);
                  console.log(`[Sound] PHASE ${debugPhase}: ‚úÖ NO CRASH ‚Äî mic is stable. rmsCount=${rmsCount}`);
                  stopSoundDetection();
                }
              }, 20000);
              
              return true;
            } catch (error) {
              console.error(`[Sound] PHASE ${debugPhase}: ‚úó Mic failed:`, error);
              return false;
            }
          }
          
          // ‚ïê‚ïê‚ïê STEP 2: Mic + model load only (no inference) ‚ïê‚ïê‚ïê
          if (debugPhase === 2) {
            console.log('[Sound] PHASE 2: Mic + model load (NO inference)');
            try {
              // v2.23.0: Minimal constraints
              soundMediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
              console.log('[Sound] PHASE 2: ‚úì Mic opened');
              
              if (!isSoundModelReady) {
                console.log('[Sound] PHASE 2: Loading YAMNet model...');
                const loaded = await loadYAMNet();
                console.log(`[Sound] PHASE 2: Model load result: ${loaded}`);
                if (!loaded) {
                  console.error('[Sound] PHASE 2: ‚úó MODEL LOAD CAUSED ISSUE or failed');
                }
              }
              
              isSoundRunning = true;
              console.log('[Sound] PHASE 2: ‚úì Mic open + model loaded ‚Äî NO inference running');
              console.log('[Sound] PHASE 2: If no crash after 20s ‚Üí model load is safe');
              
              setTimeout(() => {
                if (isSoundRunning) {
                  console.log('[Sound] PHASE 2: ‚úÖ 20s elapsed ‚Äî NO CRASH ‚Äî model load is safe');
                  stopSoundDetection();
                }
              }, 20000);
              
              return true;
            } catch (error) {
              console.error('[Sound] PHASE 2: ‚úó Failed:', error);
              return false;
            }
          }
          
          // ‚ïê‚ïê‚ïê PHASE 6 (v2.24.0): Model-only isolation ‚Äî NO mic, NO audio pipeline ‚ïê‚ïê‚ïê
          // Purpose: If this crashes ‚Üí the model/TFJS is the problem (not the mic)
          //          If this survives 20s ‚Üí the crash is in the audio pipeline
          if (debugPhase === 6) {
            console.log('[Sound] PHASE 6: Model load + warmup ONLY (NO mic, NO AudioContext)');
            try {
              if (!isSoundModelReady) {
                console.log('[Sound] PHASE 6: Loading YAMNet model...');
                const loaded = await loadYAMNet();
                console.log(`[Sound] PHASE 6: Model load result: ${loaded}`);
                if (!loaded) {
                  console.error('[Sound] PHASE 6: ‚úó MODEL LOAD FAILED');
                  return false;
                }
              }
              
              // Run 5 warmup inferences with synthetic audio data (no mic)
              console.log('[Sound] PHASE 6: Running 5 synthetic inferences...');
              for (let i = 0; i < 5; i++) {
                const fakeSamples = tf.zeros([15600]);
                const output = soundModel.predict(fakeSamples);
                if (Array.isArray(output)) { output.forEach(t => t.dispose()); } else { output.dispose(); }
                fakeSamples.dispose();
                console.log(`[Sound] PHASE 6: Synthetic inference #${i + 1}/5 OK`);
                // Small delay between inferences to simulate real timing
                await new Promise(r => setTimeout(r, 500));
              }
              
              isSoundRunning = true;
              console.log('[Sound] PHASE 6: ‚úì Model loaded + 5 inferences completed ‚Äî NO mic involved');
              console.log('[Sound] PHASE 6: Waiting 20s to confirm stability...');
              
              // Run periodic synthetic inferences for 20s to stress-test model
              let phase6InferCount = 0;
              const phase6Interval = setInterval(async () => {
                if (!isSoundRunning) { clearInterval(phase6Interval); return; }
                try {
                  const fakeSamples = tf.zeros([15600]);
                  const output = soundModel.predict(fakeSamples);
                  if (Array.isArray(output)) { output.forEach(t => t.dispose()); } else { output.dispose(); }
                  fakeSamples.dispose();
                  phase6InferCount++;
                  if (phase6InferCount % 5 === 0) {
                    console.log(`[Sound] PHASE 6: Synthetic inference #${phase6InferCount} OK, tfMemory=${JSON.stringify(tf.memory())}`);
                  }
                } catch (err) {
                  console.error(`[Sound] PHASE 6: Inference FAILED at #${phase6InferCount}:`, err);
                  clearInterval(phase6Interval);
                }
              }, 1000); // ~1 inference/sec
              
              setTimeout(() => {
                clearInterval(phase6Interval);
                if (isSoundRunning) {
                  console.log(`[Sound] PHASE 6: ‚úÖ 20s elapsed ‚Äî NO CRASH ‚Äî model is stable (${phase6InferCount} inferences). Problem is in audio pipeline.`);
                  stopSoundDetection();
                }
              }, 20000);
              
              return true;
            } catch (error) {
              console.error('[Sound] PHASE 6: ‚úó Failed:', error);
              return false;
            }
          }
          
          // ‚ïê‚ïê‚ïê Normal operation (phase 0, 3, 4) with all guards ‚ïê‚ïê‚ïê
          if (!isSoundModelReady) {
            const loaded = await loadYAMNet();
            if (!loaded) {
              console.error('[Sound] Cannot start - model not loaded');
              return false;
            }
          }
          
          // ‚ïê‚ïê‚ïê SINGLE MODE: Resolve mode from config ‚ïê‚ïê‚ïê
          let mode = config?.sensors?.sound?.mode;
          if (!mode) {
            const targets = config?.sensors?.sound?.targets || ['scream'];
            const targetToMode = { 'baby_crying': 'baby_cry', 'dog_barking': 'dog_bark', 'scream': 'help' };
            mode = targetToMode[targets[0]] || 'help';
          }
          
          const modeConfig = SOUND_MODE_CONFIG[mode];
          if (!modeConfig) {
            console.error(`[Sound] Unknown sound mode: "${mode}". Valid: baby_cry, dog_bark, help`);
            return false;
          }
          
          currentSoundMode = mode;
          currentSoundTargetLabel = modeConfig.yamnet_label;
          
          console.log('[Sound] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
          console.log(`[Sound] Sound mode: ${currentSoundMode}, target: ${currentSoundTargetLabel}`);
          console.log(`[Sound] Threshold: ${modeConfig.threshold}, Persistence: ${modeConfig.persistence}, Debounce: ${modeConfig.debounce_ms}ms`);
          const yamnetIndices = [];
          for (const [idx, info] of Object.entries(YAMNET_TARGET_CLASSES)) {
            if (info.label === currentSoundTargetLabel) yamnetIndices.push(`${idx}(${info.name})`);
          }
          console.log(`[Sound] YAMNet indices: [${yamnetIndices.join(', ')}]`);
          console.log('[Sound] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
          
          try {
            console.log('[Sound] Opening microphone...');
            // v2.23.0: Minimal constraints ‚Äî sampleRate/echoCancellation/noiseSuppression
            // caused ACCESS_VIOLATION crashes on some Windows audio drivers
            soundMediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            
            // ‚ïê‚ïê‚ïê STEP 4: Ensure no stale AudioContext exists ‚ïê‚ïê‚ïê
            if (soundAudioContext) {
              console.warn('[Sound] ‚ö†Ô∏è Closing orphaned AudioContext before creating new one');
              try { soundAudioContext.close(); } catch (_) {}
              soundAudioContext = null;
            }
            
            soundAudioContext = new AudioContext({ sampleRate: SOUND_SAMPLE_RATE });
            soundSource = soundAudioContext.createMediaStreamSource(soundMediaStream);
            soundProcessor = soundAudioContext.createScriptProcessor(4096, 1, 1);
            
            const micTrack = soundMediaStream.getAudioTracks()[0];
            console.log(`[Sound] Mic: ${micTrack?.label || 'unknown'}, AudioCtx: ${soundAudioContext.sampleRate}Hz`);
            if (soundAudioContext.sampleRate !== SOUND_SAMPLE_RATE) {
              console.warn(`[Sound] ‚ö†Ô∏è Sample rate mismatch! Expected ${SOUND_SAMPLE_RATE}Hz, got ${soundAudioContext.sampleRate}Hz`);
            }
            
            // ‚ïê‚ïê‚ïê STEP 3: Delay before connecting inference pipeline ‚ïê‚ïê‚ïê
            const delayMs = (debugPhase === 3) ? SOUND_MIC_TO_INFERENCE_DELAY_MS : 0;
            if (delayMs > 0) {
              console.log(`[Sound] PHASE 3: Waiting ${delayMs}ms before connecting inference...`);
              await new Promise(r => setTimeout(r, delayMs));
              // Re-check after delay ‚Äî might have been stopped during wait
              if (!soundMediaStream || soundMediaStream.getAudioTracks()[0]?.readyState !== 'live') {
                console.warn('[Sound] Mic track died during delay ‚Äî aborting');
                await stopSoundDetection();
                return false;
              }
            }
            
            soundProcessor.onaudioprocess = (event) => {
              if (!isSoundRunning) return;
              audioProcessCount++;
              lastAudioProcessTs = Date.now();
              if (audioProcessCount <= 3) {
                console.log(`[Sound][DBG] onaudioprocess FIRED #${audioProcessCount}`);
              }
              const inputData = event.inputBuffer.getChannelData(0);
              processSoundChunk(new Float32Array(inputData));
            };
            
            soundSource.connect(soundProcessor);
            // v2.22.0 Bonus B: Connect via silent GainNode to prevent audio feedback
            soundSilentGain = soundAudioContext.createGain();
            soundSilentGain.gain.value = 0;
            soundProcessor.connect(soundSilentGain);
            soundSilentGain.connect(soundAudioContext.destination);
            
            console.log(`[Sound] AudioContext state BEFORE resume: ${soundAudioContext.state}`);
            await soundAudioContext.resume();
            console.log(`[Sound] AudioContext state AFTER resume: ${soundAudioContext.state}`);
            
            isSoundRunning = true;
            soundPersistenceCount = {};
            soundScoreHistory = {};
            soundLastDetectionTime = {};
            audioProcessCount = 0;
            inferenceCount = 0;
            lastAudioProcessTs = 0;
            _bufferReachedLogged = false;
            
            if (_soundAudioDiagInterval) clearInterval(_soundAudioDiagInterval);
            _soundAudioDiagInterval = setInterval(() => {
              if (!isSoundRunning) {
                clearInterval(_soundAudioDiagInterval);
                _soundAudioDiagInterval = null;
                return;
              }
              const dtSinceLast = lastAudioProcessTs > 0 ? (Date.now() - lastAudioProcessTs) : -1;
              console.log(`[Sound][DBG] audioProcessCount=${audioProcessCount} dtSinceLast=${dtSinceLast}ms ctx=${soundAudioContext?.state || 'null'} ringAvail=${soundRingAvailable} inferenceCount=${inferenceCount}`);
            }, 2000);
            
            setTimeout(() => {
              console.log(`[Sound][DBG] VERIFY (1s) audioProcessCount=${audioProcessCount} ctx=${soundAudioContext?.state || 'null'}`);
            }, 1000);
            setTimeout(() => {
              console.log(`[Sound][DBG] VERIFY (2s) audioProcessCount=${audioProcessCount} ctx=${soundAudioContext?.state || 'null'}`);
            }, 2000);
            
            console.log('[Sound] ‚úì Single-mode inference started');
            return true;
          } catch (error) {
            console.error('[Sound] Failed to start microphone:', error);
            // v2.21.0: Clean up partial resources on failure
            await stopSoundDetection();
            return false;
          }
        }
        
        // ============================================================
        // SOUND: Process audio chunk ‚Üí accumulate ‚Üí infer
        // v2.22.0 Bonus A: Ring buffer (no push(...chunk), no GC pressure)
        // v2.22.0 Fix 1: Drop frames if inference still in flight
        // ============================================================
        function processSoundChunk(chunk) {
          // Write chunk into ring buffer
          for (let i = 0; i < chunk.length; i++) {
            soundRingBuffer[soundRingWriteIdx] = chunk[i];
            soundRingWriteIdx = (soundRingWriteIdx + 1) % SOUND_RING_BUFFER_SIZE;
          }
          soundRingAvailable = Math.min(soundRingAvailable + chunk.length, SOUND_RING_BUFFER_SIZE);
          
          if (!_bufferReachedLogged && soundRingAvailable >= SOUND_SAMPLES_NEEDED) {
            _bufferReachedLogged = true;
            console.log(`[Sound][DBG] ring buffer reached samplesNeeded=${SOUND_SAMPLES_NEEDED} (available=${soundRingAvailable})`);
          }
          
          if (soundRingAvailable >= SOUND_SAMPLES_NEEDED) {
            // v2.22.0 Fix 1: Single-flight ‚Äî drop frame if inference is running
            if (_inferenceInFlight) {
              _inferenceDropCount++;
              if (_inferenceDropCount <= 3 || _inferenceDropCount % 10 === 0) {
                console.log(`[Sound] DROP_FRAME #${_inferenceDropCount} ‚Äî inference in flight, skipping window`);
              }
              // Consume the samples anyway to avoid stale data buildup
              soundRingAvailable -= SOUND_SAMPLES_NEEDED;
              return;
            }
            
            // Read samples from ring buffer
            const samples = new Float32Array(SOUND_SAMPLES_NEEDED);
            let readIdx = (soundRingWriteIdx - soundRingAvailable + SOUND_RING_BUFFER_SIZE) % SOUND_RING_BUFFER_SIZE;
            for (let i = 0; i < SOUND_SAMPLES_NEEDED; i++) {
              samples[i] = soundRingBuffer[readIdx];
              readIdx = (readIdx + 1) % SOUND_RING_BUFFER_SIZE;
            }
            soundRingAvailable -= SOUND_SAMPLES_NEEDED;
            
            const rms = Math.sqrt(samples.reduce((sum, s) => sum + s * s, 0) / samples.length);
            if (rms < SOUND_RMS_THRESHOLD) {
              const nowMs = Date.now();
              if (nowMs - _rmsSkipLogTimer > 1000) {
                _rmsSkipLogTimer = nowMs;
                console.log(`[Sound] RMS_SKIP rms=${rms.toFixed(6)} < threshold=${SOUND_RMS_THRESHOLD} ‚Äî inference skipped`);
              }
              for (const label of Object.keys(soundPersistenceCount)) {
                soundPersistenceCount[label] = 0;
              }
              return;
            }
            
            runSoundInference(samples);
          }
        }
        
        // ============================================================
        // SOUND: Run YAMNet inference
        // ============================================================
        let _soundDebugCounter = 0;
        
        async function runSoundInference(samples) {
          // v2.22.0 Fix 1: Single-flight lock with try/finally
          _inferenceInFlight = true;
          inferenceCount++;
          
          try {
            const inputTensor = tf.tensor1d(samples);
            const output = soundModel.predict(inputTensor);
            
            const scores = Array.isArray(output) ? output[0] : output;
            const scoresData = await scores.data();
            
            const numClasses = 521;
            const numFrames = scoresData.length / numClasses;
            
            const avgScores = new Float32Array(numClasses);
            for (let c = 0; c < numClasses; c++) {
              let sum = 0;
              for (let f = 0; f < numFrames; f++) {
                sum += scoresData[f * numClasses + c];
              }
              avgScores[c] = sum / numFrames;
            }
            
            _soundDebugCounter++;
            const rms = Math.sqrt(samples.reduce((sum, s) => sum + s * s, 0) / samples.length);
            
            let targetScore = 0;
            const targetIndices = [];
            for (const [indexStr, classInfo] of Object.entries(YAMNET_TARGET_CLASSES)) {
              if (classInfo.label === currentSoundTargetLabel) {
                const idx = parseInt(indexStr);
                targetIndices.push(idx);
                const s = avgScores[idx];
                if (s > targetScore) targetScore = s;
              }
            }
            
            const modeConfig = SOUND_MODE_CONFIG[currentSoundMode];
            const passes = targetScore >= modeConfig.threshold;
            
            const allScored = [];
            for (let i = 0; i < numClasses; i++) {
              allScored.push({ index: i, score: avgScores[i] });
            }
            allScored.sort((a, b) => b.score - a.score);
            const top3 = allScored.slice(0, 3).map(s => `idx${s.index}:${(s.score * 100).toFixed(1)}%`).join(', ');
            const globalMaxScore = allScored[0]?.score || 0;
            
            // Step 2: Every 5 inferences log summary
            if (inferenceCount % 5 === 0) {
              console.log(`[Sound] INFERENCE_SUMMARY #${inferenceCount} | rms=${rms.toFixed(4)} | globalMaxIdx=${allScored[0]?.index} globalMax=${(globalMaxScore * 100).toFixed(1)}%`);
            }
            
            // Step 4: Target validation with explicit indices
            console.log(`[Sound] DBG #${inferenceCount} rms=${rms.toFixed(4)} | mode=${currentSoundMode} label=${currentSoundTargetLabel} targetIdx=[${targetIndices.join(',')}] bestTarget=${(targetScore * 100).toFixed(1)}% thr=${(modeConfig.threshold * 100).toFixed(0)}% ${passes ? '‚úìPASS' : '‚úóFAIL'} | Top3=[${top3}]`);
            
            processSoundScores(avgScores);
            
            inputTensor.dispose();
            if (Array.isArray(output)) {
              output.forEach(t => t.dispose());
            } else {
              output.dispose();
            }
          } catch (error) {
            console.error('[Sound] Inference error:', error);
          } finally {
            // v2.22.0 Fix 1: ALWAYS release lock, even on error
            _inferenceInFlight = false;
          }
        }
        
        // ============================================================
        // SOUND: Process scores with per-label policies
        // ============================================================
        function processSoundScores(scores) {
          const now = Date.now();
          const modeConfig = SOUND_MODE_CONFIG[currentSoundMode];
          if (!modeConfig) return;
          
          const targetLabel = currentSoundTargetLabel;
          
          // Find best score for active target label
          let bestScore = 0;
          let bestName = '';
          let bestIndex = -1;
          
          for (const [indexStr, classInfo] of Object.entries(YAMNET_TARGET_CLASSES)) {
            if (classInfo.label !== targetLabel) continue;
            const index = parseInt(indexStr);
            const score = scores[index];
            if (score > bestScore) {
              bestScore = score;
              bestName = classInfo.name;
              bestIndex = index;
            }
          }
          
          // Threshold check
          if (bestScore < modeConfig.threshold) {
            soundPersistenceCount[targetLabel] = 0;
            return;
          }
          
          // Persistence tracking
          soundPersistenceCount[targetLabel] = (soundPersistenceCount[targetLabel] || 0) + 1;
          // v2.17.0 Step 5: Log persistence rising
          console.log(`[Sound][DBG] ${currentSoundMode} persistence now = ${soundPersistenceCount[targetLabel]} (score=${(bestScore * 100).toFixed(1)}%)`);
          
          if (!soundScoreHistory[targetLabel]) soundScoreHistory[targetLabel] = [];
          soundScoreHistory[targetLabel].push(bestScore);
          if (soundScoreHistory[targetLabel].length > 10) soundScoreHistory[targetLabel].shift();
          
          if (soundPersistenceCount[targetLabel] < modeConfig.persistence) {
            console.log(`[Sound] Persistence ${soundPersistenceCount[targetLabel]}/${modeConfig.persistence} for ${currentSoundMode} - waiting`);
            return;
          }
          
          // Cooldown/debounce check
          const lastTime = soundLastDetectionTime[targetLabel] || 0;
          if (now - lastTime < modeConfig.debounce_ms) {
            const cooldownEnds = new Date(lastTime + modeConfig.debounce_ms).toISOString();
            const remaining = Math.round((modeConfig.debounce_ms - (now - lastTime)) / 1000);
            console.log(`[Sound] Cooldown active until ${cooldownEnds} (${remaining}s remaining), skipping`);
            soundPersistenceCount[targetLabel] = 0;
            return;
          }
          
          // ‚ïê‚ïê‚ïê DETECTED! ‚ïê‚ïê‚ïê
          const scoreHist = soundScoreHistory[targetLabel] || [];
          soundPersistenceCount[targetLabel] = 0;
          soundLastDetectionTime[targetLabel] = now;
          
          // Step 6: Clear single-line DETECTED log with all relevant fields
          console.log(`[Sound] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
          console.log(`[Sound] üö® SOUND DETECTED - mode=${currentSoundMode}, label=${currentSoundTargetLabel}, score=${(bestScore * 100).toFixed(1)}%, threshold=${(modeConfig.threshold * 100).toFixed(0)}%, persistence=${modeConfig.persistence}, debounce=${modeConfig.debounce_ms / 1000}s`);
          console.log(`[Sound] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê`);
          
          const audioCtxMeta = {
            score_history: scoreHist.slice(-10),
            avg_score: scoreHist.length > 0 ? scoreHist.reduce((a, b) => a + b, 0) / scoreHist.length : bestScore,
            peak_score: scoreHist.length > 0 ? Math.max(...scoreHist) : bestScore,
            consecutive_windows: modeConfig.persistence,
          };
          
          sendSoundEventToServer({
            label: currentSoundMode,
            confidence: bestScore,
            metadata: {
              yamnet_class: bestName,
              yamnet_index: bestIndex,
              sound_mode: currentSoundMode,
              audio_context: audioCtxMeta,
            },
          });
          
          // Also notify main process via IPC
          window.electronAPI?.sendMonitoringEvent?.({
            sensor_type: 'sound',
            label: currentSoundMode,
            confidence: bestScore,
            timestamp: now,
            metadata: { yamnet_class: bestName, sound_mode: currentSoundMode },
          });
        }
        
        // ============================================================
        // SOUND: Stop sound detection  (v2.22.0 ‚Äî async + await close)
        // ============================================================
        async function stopSoundDetection() {
          console.log('[Sound] Stopping... isSoundRunning=' + isSoundRunning);
          isSoundRunning = false;
          _inferenceInFlight = false; // v2.22.0: Reset inference lock
          _inferenceDropCount = 0;
          
          // Clean up diagnostic interval
          if (_soundAudioDiagInterval) {
            clearInterval(_soundAudioDiagInterval);
            _soundAudioDiagInterval = null;
          }
          
          // Disconnect audio graph nodes (order: processor ‚Üí source ‚Üí gain)
          try { if (soundProcessor) { soundProcessor.onaudioprocess = null; soundProcessor.disconnect(); } } catch (_) {}
          soundProcessor = null;
          
          try { if (soundSilentGain) { soundSilentGain.disconnect(); } } catch (_) {}
          soundSilentGain = null;
          
          try { if (soundSource) { soundSource.disconnect(); } } catch (_) {}
          soundSource = null;
          
          // Stop mic tracks
          if (soundMediaStream) {
            soundMediaStream.getTracks().forEach(track => {
              try { track.stop(); console.log('[Sound] Stopped mic track:', track.label); } catch (_) {}
            });
            soundMediaStream = null;
          }
          
          // v2.22.0 Fix 3: AWAIT audioContext.close() to prevent orphaned contexts
          if (soundAudioContext) {
            try {
              if (soundAudioContext.state !== 'closed') {
                await soundAudioContext.close();
                console.log('[Sound] ‚úì AudioContext closed (awaited)');
              }
            } catch (e) {
              console.warn('[Sound] AudioContext close error (non-fatal):', e.message);
            }
            soundAudioContext = null;
          }
          
          // Full state reset
          soundRingBuffer = new Float32Array(SOUND_RING_BUFFER_SIZE);
          soundRingWriteIdx = 0;
          soundRingAvailable = 0;
          soundPersistenceCount = {};
          soundScoreHistory = {};
          soundLastDetectionTime = {};
          audioProcessCount = 0;
          inferenceCount = 0;
          lastAudioProcessTs = 0;
          _bufferReachedLogged = false;
          console.log('[Sound] ‚úì Stopped ‚Äî all resources released and state reset');
        }
        
        // ============================================================
        // Detection loop
        // ============================================================
        let lastFrameTime = 0;
        const DETECTION_INTERVAL_MS = 200; // 5 FPS
        let detectionLoopTimer = null;
        
        let _detectionTickCount = 0;
        
        function runDetectionLoop() {
          // IMPORTANT: Do NOT rely on requestAnimationFrame.
          // When the Electron window is hidden/minimized to Tray, Chromium may stop/throttle rAF,
          // which makes motion detection only run when the window is visible.
          if (!detectionLoopActive || !objectDetector || !videoElement) {
            console.warn('[Monitoring] runDetectionLoop ABORTED - missing:', {
              detectionLoopActive, objectDetector: !!objectDetector, videoElement: !!videoElement
            });
            return;
          }

          if (detectionLoopTimer) {
            clearTimeout(detectionLoopTimer);
            detectionLoopTimer = null;
          }

          const tick = () => {
            if (!detectionLoopActive || !objectDetector || !videoElement) return;

            const now = performance.now();

            if (now - lastFrameTime >= DETECTION_INTERVAL_MS) {
              lastFrameTime = now;
              _detectionTickCount++;

              // Periodic heartbeat log every 60 ticks (~12 seconds at 5 FPS)
              if (_detectionTickCount % 60 === 0) {
                console.log(`[Monitoring] üíì Detection heartbeat: ${_detectionTickCount} ticks, video.readyState=${videoElement.readyState}, paused=${videoElement.paused}`);
              }

              if (videoElement.readyState >= 2 && !videoElement.paused) {
                try {
                  const detections = objectDetector.detectForVideo(videoElement, now);
                  processDetections(detections.detections);
                } catch (e) {
                  if (!window._lastDetectError || (now - window._lastDetectError > 30000)) {
                    console.error('[Monitoring] ‚úó detectForVideo error:', e.message || e);
                    window._lastDetectError = now;
                  }
                }
              } else if (_detectionTickCount % 60 === 1) {
                console.warn(`[Monitoring] ‚ö†Ô∏è Video not ready: readyState=${videoElement.readyState}, paused=${videoElement.paused}`);
              }
            }

            detectionLoopTimer = setTimeout(tick, DETECTION_INTERVAL_MS);
          };

          tick();
        }
        
        function processDetections(detections) {
          const nowMs = Date.now();
          const targets = currentConfig?.sensors?.motion?.targets || ['person', 'animal', 'vehicle'];
          const threshold = currentConfig?.sensors?.motion?.confidence_threshold || 0.6;
          const debounceMs = currentConfig?.sensors?.motion?.debounce_ms || DEBOUNCE_MS;
          
          for (const detection of detections) {
            if (!detection.categories || detection.categories.length === 0) continue;
            
            const category = detection.categories[0];
            const rawLabel = category.categoryName.toLowerCase();
            const confidence = category.score;
            
            const mappedLabel = LABEL_MAPPING[rawLabel];
            if (!mappedLabel) continue;
            if (!targets.includes(mappedLabel)) continue;
            if (confidence < threshold) continue;
            
            // Debounce check
            const lastTime = lastDetectionTime[mappedLabel] || 0;
            if (nowMs - lastTime < debounceMs) continue;
            
            lastDetectionTime[mappedLabel] = nowMs;
            
            console.log(`[Monitoring] üéØ Detected: ${rawLabel} ‚Üí ${mappedLabel} (${(confidence * 100).toFixed(1)}%)`);
            
            // Send to server
            sendEventToServer({
              label: mappedLabel,
              confidence: confidence,
              metadata: {
                raw_label: rawLabel,
                bounding_box: detection.boundingBox ? {
                  x: detection.boundingBox.originX,
                  y: detection.boundingBox.originY,
                  width: detection.boundingBox.width,
                  height: detection.boundingBox.height,
                } : null,
              },
            });
            
            // Also send to main process via IPC
            window.electronAPI?.sendMonitoringEvent?.({
              sensor_type: 'motion',
              label: mappedLabel,
              confidence: confidence,
              timestamp: nowMs,
            });
          }
        }
        
        // ============================================================
        // Stop monitoring resources (for clean-start)
        // ============================================================
        function stopMonitoringResources(options = {}) {
          const silent = options.silent || false;
          if (!silent) console.log('[Monitoring] Releasing existing resources...');
          
          detectionLoopActive = false;

          if (detectionLoopTimer) {
            clearTimeout(detectionLoopTimer);
            detectionLoopTimer = null;
          }
          
          if (monitoringStream) {
            monitoringStream.getTracks().forEach(track => {
              track.stop();
              if (!silent) console.log('[Monitoring] Stopped track:', track.kind, track.label);
            });
            monitoringStream = null;
          }
          
          if (videoElement && videoElement.srcObject) {
            videoElement.srcObject = null;
          }
          
          // Also stop sound detection on clean-start
          if (isSoundRunning) {
            stopSoundDetection();
          }
        }
        
        // ============================================================
        // Helper: Try getUserMedia with constraints
        // ============================================================
        async function tryGetUserMedia(constraints, label) {
          console.log(`[Monitoring] Trying getUserMedia: ${label}...`);
          try {
            const stream = await navigator.mediaDevices.getUserMedia(constraints);
            console.log(`[Monitoring] ‚úì getUserMedia succeeded (${label})`);
            return stream;
          } catch (err) {
            console.warn(`[Monitoring] ‚úó getUserMedia failed (${label}):`, err.name, err.message);
            return null;
          }
        }

        /**
         * v2.13.0: Camera acquisition with retry + backoff
         * Handles NotReadableError when camera is busy (e.g., clip recording in progress)
         * Retries up to MAX_RETRIES times with increasing delays
         */
        async function acquireCameraWithRetry(maxRetries = 4, baseDelayMs = 3000) {
          for (let attempt = 1; attempt <= maxRetries; attempt++) {
            console.log(`[Monitoring] üì∑ Camera acquisition attempt ${attempt}/${maxRetries}...`);
            
            let stream = await tryGetUserMedia(
              { video: { width: { ideal: 640 }, height: { ideal: 480 }, frameRate: { ideal: 15 } }, audio: false },
              `preferred 640x480@15 (attempt ${attempt})`
            );
            
            if (!stream) {
              stream = await tryGetUserMedia(
                { video: { width: { max: 1280 }, height: { max: 720 } }, audio: false },
                `relaxed max 1280x720 (attempt ${attempt})`
              );
            }
            
            if (!stream) {
              stream = await tryGetUserMedia(
                { video: true, audio: false },
                `minimal video:true (attempt ${attempt})`
              );
            }
            
            if (stream) {
              console.log(`[Monitoring] ‚úì Camera acquired on attempt ${attempt}`);
              return stream;
            }
            
            if (attempt < maxRetries) {
              const delay = baseDelayMs * attempt; // 3s, 6s, 9s, 12s
              console.warn(`[Monitoring] ‚è≥ Camera busy - retrying in ${delay/1000}s (clip recording may be active)...`);
              await new Promise(resolve => setTimeout(resolve, delay));
            }
          }
          
          console.error(`[Monitoring] ‚úó All ${maxRetries} camera acquisition attempts failed`);
          return null;
        }
        
        // ============================================================
        // Build bilingual error message
        // ============================================================
        function buildMonitoringErrorMessage(err) {
          const name = err.name || 'Error';
          const msg = err.message || 'Unknown';
          
          if (name === 'NotReadableError') {
            return `Camera busy/unavailable (NotReadableError): ${msg} | ◊û◊¶◊ú◊û◊î ◊™◊§◊ï◊°◊î/◊ú◊ê ◊ñ◊û◊ô◊†◊î ‚Äì ◊°◊í◊ï◊® ◊™◊ï◊õ◊†◊ï◊™ ◊ê◊ó◊®◊ï◊™ ◊©◊û◊©◊™◊û◊©◊ï◊™ ◊ë◊û◊¶◊ú◊û◊î ◊ï◊†◊°◊î ◊©◊ï◊ë`;
          }
          if (name === 'NotAllowedError') {
            return `Camera permission denied (NotAllowedError): ${msg} | ◊î◊®◊©◊ê◊™ ◊û◊¶◊ú◊û◊î ◊†◊ì◊ó◊™◊î ‚Äì ◊ê◊©◊® ◊î◊®◊©◊ê◊î ◊ë◊î◊í◊ì◊®◊ï◊™ ◊î◊û◊¢◊®◊õ◊™`;
          }
          if (name === 'OverconstrainedError') {
            return `Camera constraints not supported (OverconstrainedError): ${msg} | ◊î◊í◊ì◊®◊ï◊™ ◊î◊û◊¶◊ú◊û◊î ◊ú◊ê ◊†◊™◊û◊õ◊ï◊™ ‚Äì ◊†◊°◊î ◊û◊¶◊ú◊û◊î ◊ê◊ó◊®◊™`;
          }
          if (name === 'NotFoundError') {
            return `No camera found (NotFoundError): ${msg} | ◊ú◊ê ◊†◊û◊¶◊ê◊î ◊û◊¶◊ú◊û◊î ‚Äì ◊ó◊ë◊® ◊û◊¶◊ú◊û◊î ◊ú◊û◊ó◊©◊ë`;
          }
          return `Camera error (${name}): ${msg} | ◊©◊í◊ô◊ê◊™ ◊û◊¶◊ú◊û◊î: ${msg}`;
        }
        
        // ============================================================
        // Start monitoring (v2.3.2 - with fallbacks + clean-start)
        // ============================================================
        async function startMonitoring(config) {
          console.log('[Monitoring] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
          console.log('[Monitoring] Starting monitoring v2.6.0 with config:', JSON.stringify(config, null, 2));
          console.log('[Monitoring] ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê');
          currentConfig = config;
          
          // CLEAN-START: Release any existing resources first
          stopMonitoringResources({ silent: true });
          
          // CRITICAL: Reset startup timestamp for grace period
          monitoringStartedAt = null;
          
          const motionEnabled = config?.sensors?.motion?.enabled ?? false;
          const soundEnabled = config?.sensors?.sound?.enabled ?? false;
          
          console.log('[Monitoring] Sensor config:', { motionEnabled, soundEnabled });
          
          try {
            let motionStarted = false;
            let soundStarted = false;
            
            // ============================================================
            // MOTION: Only activate camera if motion detection is enabled
            // ============================================================
            if (motionEnabled) {
              console.log('[Monitoring] Motion enabled - requesting camera access with retry...');
              
              // v2.13.0: Use retry mechanism for camera busy scenarios (clip recording)
              monitoringStream = await acquireCameraWithRetry(4, 3000);
              
              if (!monitoringStream) {
                console.error('[Monitoring] ‚úó Camera unavailable after all retries');
                // STRUCTURAL ISOLATION: Camera failure does NOT throw
                // Motion simply won't start, but system continues
                console.warn('[Monitoring] ‚ö†Ô∏è Motion detection SKIPPED - camera unavailable');
              } else {
                console.log('[Monitoring] ‚úì Camera acquired - LED should be ON now');
                
                // Create hidden video element
                videoElement = document.getElementById('monitoring-video');
                if (!videoElement) {
                  videoElement = document.createElement('video');
                  videoElement.id = 'monitoring-video';
                  videoElement.autoplay = true;
                  videoElement.muted = true;
                  videoElement.playsInline = true;
                  videoElement.style.position = 'absolute';
                  videoElement.style.left = '-9999px';
                  videoElement.style.width = '640px';
                  videoElement.style.height = '480px';
                  document.body.appendChild(videoElement);
                }
                
                videoElement.srcObject = monitoringStream;
                await videoElement.play();
                
                // Initialize MediaPipe detector if not done
                if (!isDetectorReady) {
                  const mpResult = await loadMediaPipe();
                  console.log('[Monitoring] MediaPipe load result:', mpResult);
                }
                
                // Start detection loop
                if (isDetectorReady) {
                  detectionLoopActive = true;
                  motionStarted = true;
                  console.log('[Monitoring] ‚úì Motion detection loop started');
                  runDetectionLoop();
                }
              }
            } else {
              console.log('[Monitoring] Motion DISABLED - camera stays OFF üì∑üö´');
            }
            
            // ============================================================
            // SOUND: Independent, non-blocking subsystem
            // Sound failure must NEVER block ACK or affect motion
            // ============================================================
            let soundError = null;
            if (soundEnabled) {
              try {
                console.log('[Monitoring] Sound enabled - starting YAMNet microphone detection...');
                const soundOk = await startSoundDetection(config);
                if (soundOk) {
                  soundStarted = true;
                  console.log('[Monitoring] ‚úì Sound detection started (YAMNet + microphone)');
                } else {
                  soundError = 'Sound detection failed to start (model or microphone unavailable)';
                  console.error('[Monitoring] ‚úó Sound detection failed to start');
                }
              } catch (soundErr) {
                soundError = soundErr.message || 'Unknown sound error';
                console.error('[Monitoring] ‚úó Sound detection threw error (non-blocking):', soundErr);
              }
            }
            
            // v2.13.0: Mark monitoring active if ANY sensor started
            // Even if camera failed, sound can still work (and vice versa)
            const anySensorStarted = motionStarted || soundStarted;
            isMonitoring = anySensorStarted;
            if (anySensorStarted) {
              monitoringStartedAt = Date.now();
              console.log('[Monitoring] ‚úì Grace period started (10s)');
            } else {
              console.warn('[Monitoring] ‚ö†Ô∏è No sensors started - monitoring inactive');
            }
            
            // Build comprehensive error report
            const errors = {};
            if (motionEnabled && !motionStarted) errors.motion = 'Camera unavailable after retries';
            if (soundError) errors.sound = soundError;
            
            // CRITICAL: ACK is ALWAYS sent, regardless of individual sensor status
            const ackPayload = {
              motion: motionStarted,
              sound: soundStarted,
              errors: Object.keys(errors).length > 0 ? errors : undefined,
            };
            console.log('[Monitoring] Sending ACK to main process...');
            window.electronAPI?.notifyMonitoringStarted?.(ackPayload);
            console.log('[Monitoring] ‚úì Sent monitoring-started ACK:', JSON.stringify(ackPayload));
            
            // v2.16.1: Canary diagnostic ‚Äî fires 3s after ACK regardless of isSoundRunning
            if (soundStarted) {
              setTimeout(() => {
                console.log(`[Sound] CANARY (3s post-ACK) isSoundRunning=${isSoundRunning} | audioProcessCount=${audioProcessCount} | inferenceCount=${inferenceCount} | ctxState=${soundAudioContext?.state || 'null'} | diagIntervalActive=${!!_soundAudioDiagInterval} | ringAvail=${soundRingAvailable}`);
                if (audioProcessCount === 0) {
                  console.warn('[Sound] ‚ö†Ô∏è CANARY: onaudioprocess never fired! ScriptProcessor may be disconnected.');
                  // Attempt reconnect
                  try {
                    if (soundSource && soundProcessor && soundAudioContext) {
                      console.log('[Sound] CANARY: Attempting ScriptProcessor reconnect...');
                      soundSource.disconnect();
                      soundProcessor.disconnect();
                      if (soundSilentGain) try { soundSilentGain.disconnect(); } catch(_){}
                      soundSource.connect(soundProcessor);
                      soundSilentGain = soundAudioContext.createGain();
                      soundSilentGain.gain.value = 0;
                      soundProcessor.connect(soundSilentGain);
                      soundSilentGain.connect(soundAudioContext.destination);
                      console.log('[Sound] CANARY: Reconnected ScriptProcessor via GainNode(0)');
                    }
                  } catch (e) {
                    console.error('[Sound] CANARY: Reconnect failed:', e);
                  }
                }
              }, 3000);
              
              // Second canary at 6s to check if reconnect worked
              setTimeout(() => {
                console.log(`[Sound] CANARY2 (6s post-ACK) isSoundRunning=${isSoundRunning} | audioProcessCount=${audioProcessCount} | inferenceCount=${inferenceCount}`);
              }, 6000);
              
              // v2.17.0: Third canary at 10s ‚Äî full pipeline state dump
              setTimeout(() => {
                console.log(`[Sound] CANARY3 (10s post-ACK) isSoundRunning=${isSoundRunning} | audioProcessCount=${audioProcessCount} | inferenceCount=${inferenceCount} | ctxState=${soundAudioContext?.state || 'null'} | ringAvail=${soundRingAvailable} | dtSinceLast=${lastAudioProcessTs > 0 ? (Date.now() - lastAudioProcessTs) + 'ms' : 'never'}`);
              }, 10000);
            }
            
            // Update sensor status indicator in UI
            updateSensorIndicator({ motion: motionStarted, sound: soundStarted });
          } catch (error) {
            console.error('[Monitoring] ‚úó Start failed:', error);
            console.error('[Monitoring] Error name:', error.name);
            console.error('[Monitoring] Error message:', error.message);
            
            const bilingual = buildMonitoringErrorMessage(error);
            console.error('[Monitoring] Full error:', bilingual);
            
            // CRITICAL: Even on total failure, send ACK not error
            // to prevent main process timeout
            window.electronAPI?.notifyMonitoringStarted?.({
              motion: false,
              sound: false,
              errors: { system: bilingual },
            });
          }
        }
        
        // ============================================================
        // Stop monitoring
        // ============================================================
        function stopMonitoring() {
          console.log('[Monitoring] Stopping monitoring...');
          
          detectionLoopActive = false;
          isMonitoring = false;
          monitoringStartedAt = null;

          if (detectionLoopTimer) {
            clearTimeout(detectionLoopTimer);
            detectionLoopTimer = null;
          }
          
          if (monitoringStream) {
            monitoringStream.getTracks().forEach(track => {
              track.stop();
              console.log('[Monitoring] Stopped track:', track.kind);
            });
            monitoringStream = null;
          }
          
          if (videoElement) {
            videoElement.srcObject = null;
          }
          
          // Stop sound detection
          stopSoundDetection();
          
          window.electronAPI?.notifyMonitoringStopped?.();
          updateSensorIndicator({ motion: false, sound: false });
          console.log('[Monitoring] ‚úì Monitoring stopped');
        }
        
        // ============================================================
        // IPC Listeners
        // ============================================================
        if (window.electronAPI?.onStartMonitoring) {
          window.electronAPI.onStartMonitoring(async (config) => {
            console.log('[Monitoring] Received START_MONITORING command');
            
            // Get device credentials from main process store
            // They should be passed in config or we need to get them separately
            deviceId = config?.device_id || localStorage.getItem('device_id');
            deviceAuthToken = config?.device_auth_token || localStorage.getItem('device_auth_token');
            
            if (!deviceId || !deviceAuthToken) {
              console.warn('[Monitoring] ‚ö†Ô∏è Missing device credentials - events won\'t be reported to server');
            }
            
            await startMonitoring(config);
          });
          console.log('[Monitoring] ‚úì onStartMonitoring listener registered');
        }
        
        if (window.electronAPI?.onStopMonitoring) {
          window.electronAPI.onStopMonitoring(() => {
            console.log('[Monitoring] Received STOP_MONITORING command');
            stopMonitoring();
          });
          console.log('[Monitoring] ‚úì onStopMonitoring listener registered');
        }
        
        // ============================================================
        // Clip Recording - triggered by Main after AI validates event
        // ============================================================
        if (window.electronAPI?.onStartClipRecording) {
          window.electronAPI.onStartClipRecording(async ({ eventId, durationSeconds }) => {
            console.log(`[ClipRecorder] Recording requested: event=${eventId}, duration=${durationSeconds}s`);
            
            if (!monitoringStream || !videoElement) {
              console.warn('[ClipRecorder] No active monitoring stream, skipping clip');
              return;
            }
            
            try {
              const recorder = new MediaRecorder(monitoringStream, {
                mimeType: 'video/webm;codecs=vp9',
                videoBitsPerSecond: 2500000,
              });
              
              const chunks = [];
              
              recorder.ondataavailable = (e) => {
                if (e.data.size > 0) chunks.push(e.data);
              };
              
              recorder.onstop = async () => {
                console.log(`[ClipRecorder] Recording stopped, ${chunks.length} chunks`);
                const blob = new Blob(chunks, { type: 'video/webm' });
                
                // Convert to base64
                const reader = new FileReader();
                reader.onloadend = async () => {
                  const base64 = reader.result.split(',')[1]; // strip data:... prefix
                  const now = new Date();
                  const pad = (n) => String(n).padStart(2, '0');
                  const dateStr = `${now.getFullYear()}-${pad(now.getMonth()+1)}-${pad(now.getDate())}`;
                  const timeStr = `${pad(now.getHours())}-${pad(now.getMinutes())}-${pad(now.getSeconds())}`;
                  const filename = `clip_${dateStr}_${timeStr}_${eventId.substring(0,8)}.webm`;
                  
                  console.log(`[ClipRecorder] Saving clip: ${filename} (${(blob.size / 1024 / 1024).toFixed(2)} MB)`);
                  
                  const result = await window.electronAPI.saveClip({
                    filename,
                    base64Data: base64,
                    eventId,
                    durationSeconds,
                  });
                  
                  if (result.success) {
                    console.log(`[ClipRecorder] ‚úì Clip saved: ${result.filepath}`);
                    window.electronAPI.notifyClipRecorded({
                      eventId,
                      filename,
                      durationSeconds,
                      filepath: result.filepath,
                    });
                  } else {
                    console.error(`[ClipRecorder] ‚úó Save failed:`, result.error);
                  }
                };
                reader.readAsDataURL(blob);
              };
              
              recorder.start();
              console.log(`[ClipRecorder] ‚úì Recording started (${durationSeconds}s)`);
              
              setTimeout(() => {
                if (recorder.state === 'recording') {
                  recorder.stop();
                  console.log('[ClipRecorder] Timer expired, stopping recorder');
                }
              }, durationSeconds * 1000);
              
            } catch (err) {
              console.error('[ClipRecorder] Recording error:', err);
            }
          });
          console.log('[Monitoring] ‚úì onStartClipRecording listener registered');
        }
        
        // v2.20.0: MediaPipe is NO LONGER pre-loaded on startup.
        // It will be loaded on-demand only when motion detection is actually enabled.
        // This prevents ACCESS_VIOLATION (0xC0000005) crashes during sound-only monitoring,
        // because MediaPipe WASM initialization can trigger GPU probing even with CPU delegate.
        console.log('[Monitoring] MediaPipe deferred ‚Äî will load on first motion-enable request');
        
        console.log('[Monitoring] ‚úì Monitoring system initialized');
      })();
      // ============================================================
      // CONFIG (SUPABASE_URL is already defined in renderer-webrtc.js)
      // ============================================================

      // ============================================================
      // I18N
      // ============================================================
      const STRINGS = {
        en: {
          dir: "ltr",
          title: "Connect Camera",
          subtitle: "Enter the 6-digit code from your dashboard",
          codeLabel: "Pairing Code",
          pasteText: "Paste Code",
          pasteHint: "Tip: You can paste the full code and we will fill it automatically.",
          connect: "Connect",
          successTitle: "Connected Successfully",
          successSubtitle: "Your camera is now linked to your account",
          live: "LIVE - Streaming",
          bgTitle: "Background Operation",
          bgText:
            "The camera will continue to work in the background even when this window is closed or minimized.\nTo reopen the app, click the AIGuard icon near the clock.",
          minimize: "Minimize to Background",
          exit: "Exit Application",
          exitWarning: "Warning: Exiting will stop the camera service",
          errClipboard: "Could not read clipboard. Please paste the code manually.",
          errInvalid: "Please enter a 6-digit code.",
          errVerify: "Invalid or expired code.",
          errGeneric: "Something went wrong. Please try again.",
          connecting: "Connecting‚Ä¶",
          // Away Mode
          awayModeTitle: "Away Mode Active",
          awayModeText: "Camera is monitoring. Display will turn off.",
          userReturnedTitle: "Welcome Back",
          userReturnedMessage: "You have returned. Would you like to disable Away Mode?",
          disableAwayMode: "Disable Away Mode",
          keepAwayMode: "Keep Away Mode"
        },
        he: {
          dir: "rtl",
          title: "◊ó◊ô◊ë◊ï◊® ◊û◊¶◊ú◊û◊î",
          subtitle: "◊î◊ñ◊ü/◊ô ◊ß◊ï◊ì ◊ë◊ü 6 ◊°◊§◊®◊ï◊™ ◊û◊î◊ì◊©◊ë◊ï◊®◊ì",
          codeLabel: "◊ß◊ï◊ì ◊¶◊ô◊û◊ï◊ì",
          pasteText: "◊î◊ì◊ë◊ß ◊ß◊ï◊ì",
          pasteHint: "◊ò◊ô◊§: ◊ê◊§◊©◊® ◊ú◊î◊ì◊ë◊ô◊ß ◊ê◊™ ◊õ◊ú ◊î◊ß◊ï◊ì ◊ï◊ê◊†◊ó◊†◊ï ◊†◊û◊ú◊ê ◊ê◊ï◊™◊ï ◊ê◊ï◊ò◊ï◊û◊ò◊ô◊™.",
          connect: "◊î◊™◊ó◊ë◊®",
          successTitle: "◊ó◊ô◊ë◊ï◊® ◊î◊¶◊ú◊ô◊ó",
          successSubtitle: "◊î◊û◊¶◊ú◊û◊î ◊ß◊ï◊©◊®◊î ◊ú◊ó◊©◊ë◊ï◊ü ◊©◊ú◊ö",
          live: "LIVE - ◊©◊ô◊ì◊ï◊® ◊§◊¢◊ô◊ú",
          bgTitle: "◊§◊¢◊ô◊ú◊ï◊™ ◊ë◊®◊ß◊¢",
          bgText:
            "◊î◊û◊¶◊ú◊û◊î ◊™◊û◊©◊ô◊ö ◊ú◊¢◊ë◊ï◊ì ◊ë◊®◊ß◊¢ ◊í◊ù ◊ê◊ù ◊î◊ó◊ú◊ï◊ü ◊†◊°◊í◊® ◊ê◊ï ◊û◊û◊ï◊ñ◊¢◊®.\n◊õ◊ì◊ô ◊ú◊§◊™◊ï◊ó ◊©◊ï◊ë, ◊ú◊ó◊•/◊ô ◊¢◊ú ◊î◊ê◊ô◊ô◊ß◊ï◊ü ◊ú◊ô◊ì ◊î◊©◊¢◊ï◊ü.",
          minimize: "◊û◊ñ◊¢◊® ◊ú◊®◊ß◊¢",
          exit: "◊ô◊¶◊ô◊ê◊î ◊û◊î◊ê◊§◊ú◊ô◊ß◊¶◊ô◊î",
          exitWarning: "◊ê◊ñ◊î◊®◊î: ◊ô◊¶◊ô◊ê◊î ◊™◊¢◊¶◊ï◊® ◊ê◊™ ◊©◊ô◊®◊ï◊™ ◊î◊û◊¶◊ú◊û◊î",
          errClipboard: "◊ú◊ê ◊†◊ô◊™◊ü ◊ú◊ß◊®◊ï◊ê ◊û◊î◊ú◊ï◊ó. ◊†◊ê ◊ú◊î◊ì◊ë◊ô◊ß ◊ô◊ì◊†◊ô◊™.",
          errInvalid: "◊†◊ê ◊ú◊î◊ñ◊ô◊ü ◊ß◊ï◊ì ◊ë◊ü 6 ◊°◊§◊®◊ï◊™.",
          errVerify: "◊ß◊ï◊ì ◊ú◊ê ◊™◊ß◊ô◊ü ◊ê◊ï ◊§◊í ◊™◊ï◊ß◊£.",
          errGeneric: "◊û◊©◊î◊ï ◊î◊©◊™◊ë◊©. ◊†◊°◊î/◊ô ◊©◊ï◊ë.",
          connecting: "◊û◊™◊ó◊ë◊®‚Ä¶",
          // Away Mode
          awayModeTitle: "◊û◊¶◊ë ◊û◊®◊ï◊ó◊ß ◊§◊¢◊ô◊ú",
          awayModeText: "◊î◊û◊¶◊ú◊û◊î ◊ë◊û◊¶◊ë ◊†◊ô◊ò◊ï◊®. ◊î◊û◊°◊ö ◊ô◊õ◊ë◊î.",
          userReturnedTitle: "◊ë◊®◊ï◊ö ◊©◊ï◊ë◊ö",
          userReturnedMessage: "◊ó◊ñ◊®◊™ ◊î◊ë◊ô◊™◊î. ◊î◊ê◊ù ◊ú◊õ◊ë◊ï◊™ ◊ê◊™ ◊û◊¶◊ë ◊û◊®◊ï◊ó◊ß?",
          disableAwayMode: "◊õ◊ë◊î ◊û◊¶◊ë ◊û◊®◊ï◊ó◊ß",
          keepAwayMode: "◊î◊©◊ê◊® ◊û◊¶◊ë ◊û◊®◊ï◊ó◊ß"
        }
      };

      let currentLang = "en";

      function setLanguage(lang) {
        currentLang = lang;
        const t = STRINGS[lang];

        document.documentElement.lang = lang;
        document.documentElement.dir = t.dir;

        document.getElementById("lang-en").classList.toggle("active", lang === "en");
        document.getElementById("lang-he").classList.toggle("active", lang === "he");

        document.getElementById("title").textContent = t.title;
        document.getElementById("subtitle").textContent = t.subtitle;
        document.getElementById("code-label").textContent = t.codeLabel;
        document.getElementById("paste-text").textContent = t.pasteText;
        document.getElementById("paste-hint").textContent = t.pasteHint;
        document.getElementById("connect-btn").textContent = t.connect;

        document.getElementById("success-title").textContent = t.successTitle;
        document.getElementById("success-subtitle").textContent = t.successSubtitle;
        document.getElementById("live-text").textContent = t.live;
        document.getElementById("bg-title").textContent = t.bgTitle;
        document.getElementById("bg-text").textContent = t.bgText;
        document.getElementById("minimize-text").textContent = t.minimize;
        document.getElementById("exit-text").textContent = t.exit;
        document.getElementById("exit-warning").textContent = t.exitWarning;
      }

      // ============================================================
      // PAIRING UI HELPERS
      // ============================================================
      const codeInputs = Array.from(document.querySelectorAll(".code-input"));
      const errorEl = document.getElementById("error");
      const connectBtn = document.getElementById("connect-btn");
      const pasteBtn = document.getElementById("paste-btn");

      function showError(msg) {
        errorEl.textContent = msg;
        errorEl.style.display = "block";
      }

      function clearError() {
        errorEl.textContent = "";
        errorEl.style.display = "none";
      }

      function getCode() {
        return codeInputs.map((i) => (i.value || "").replace(/\D/g, "")).join("").slice(0, 6);
      }

      function setCode(code) {
        const digits = String(code || "")
          .replace(/\D/g, "")
          .slice(0, 6)
          .split("");

        for (let idx = 0; idx < codeInputs.length; idx++) {
          codeInputs[idx].value = digits[idx] || "";
        }

        const nextIndex = Math.min(digits.length, codeInputs.length - 1);
        codeInputs[nextIndex].focus();
      }

      async function readClipboardText() {
        // Electron usually supports navigator.clipboard in secure contexts,
        // but fallback to prompt for reliability.
        if (navigator.clipboard && navigator.clipboard.readText) {
          return navigator.clipboard.readText();
        }
        return null;
      }

      function showSuccessScreen() {
        document.getElementById("pairing-screen").style.display = "none";
        document.getElementById("success-screen").style.display = "block";
      }

      function setLiveIndicator(isLive) {
        document.getElementById("live-indicator").classList.toggle("active", !!isLive);
      }

      // ============================================================
      // INPUT BEHAVIOR (auto-advance, backspace)
      // ============================================================
      codeInputs.forEach((input, idx) => {
        input.addEventListener("input", (e) => {
          clearError();
          const v = String(e.target.value || "").replace(/\D/g, "");
          e.target.value = v.slice(-1);
          if (e.target.value && idx < codeInputs.length - 1) {
            codeInputs[idx + 1].focus();
          }
        });

        input.addEventListener("keydown", (e) => {
          if (e.key === "Backspace" && !input.value && idx > 0) {
            codeInputs[idx - 1].focus();
          }
          if (e.key === "Enter") {
            connectBtn.click();
          }
        });

        input.addEventListener("paste", (e) => {
          const text = (e.clipboardData || window.clipboardData)?.getData("text") || "";
          const digits = text.replace(/\D/g, "").slice(0, 6);
          if (digits.length) {
            e.preventDefault();
            setCode(digits);
          }
        });
      });

      pasteBtn.addEventListener("click", async () => {
        clearError();
        const t = STRINGS[currentLang];
        try {
          const text = (await readClipboardText()) || prompt(t.pasteText) || "";
          const digits = String(text).replace(/\D/g, "").slice(0, 6);
          if (!digits) {
            showError(t.errClipboard);
            return;
          }
          setCode(digits);
        } catch (e) {
          showError(t.errClipboard);
        }
      });

      // ============================================================
      // PAIRING (Edge Function)
      // ============================================================
      async function verifyPairingCode(code) {
        const res = await fetch(`${SUPABASE_URL}/functions/v1/verify-pairing-code`, {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ code }),
        });

        const data = await res.json().catch(() => ({}));
        if (!res.ok) {
          const err = data?.error || "verify_failed";
          throw new Error(err);
        }
        return data;
      }

      connectBtn.addEventListener("click", async () => {
        clearError();
        const t = STRINGS[currentLang];

        const code = getCode();
        if (code.length !== 6) {
          showError(t.errInvalid);
          return;
        }

        const originalText = connectBtn.textContent;
        connectBtn.disabled = true;
        connectBtn.textContent = t.connecting;

        try {
          const data = await verifyPairingCode(code);

          // IMPORTANT: keep snake_case for IPC sync
          console.log('[UI] Pairing verified. electronAPI.loginUser available:', !!window.electronAPI?.loginUser);

          if (window.electronAPI?.loginUser) {
            console.log('[UI] Sending login-user IPC to main...', {
              profile_id: data.profile_id,
              device_id: data.device_id,
              has_session_token: !!data.session_token,
            });
            window.electronAPI.loginUser({
              profile_id: data.profile_id,
              session_token: data.session_token,
              device_id: data.device_id,
            });
          } else {
            console.warn(
              '[UI] WARNING: electronAPI.loginUser is missing. Main process will NOT receive profile_id/device_id, so Auto-Away cannot run.'
            );
          }

          showSuccessScreen();
        } catch (e) {
          const msg = String(e?.message || "");
          if (msg.toLowerCase().includes("invalid") || msg.toLowerCase().includes("expired")) {
            showError(t.errVerify);
          } else {
            showError(t.errGeneric);
          }
        } finally {
          connectBtn.disabled = false;
          connectBtn.textContent = originalText;
        }
      });

      // ============================================================
      // SUCCESS SCREEN ACTIONS
      // ============================================================
      document.getElementById("minimize-btn").addEventListener("click", () => {
        window.electronAPI?.minimizeToTray?.();
      });

      document.getElementById("exit-btn").addEventListener("click", () => {
        window.electronAPI?.exitApp?.();
      });

      // ============================================================
      // IPC -> UI live indicator (NOTE: actual WebRTC logic is in renderer-webrtc.js)
      // ============================================================
      // DO NOT register onStartLiveView/onStopLiveView here - it would override 
      // the WebRTC handlers in renderer-webrtc.js. The UI indicator is now 
      // handled via a separate approach if needed.

      // Allow main process to switch to success screen (auto-login)
      if (window.electronAPI?.onShowSuccessScreen) {
        window.electronAPI.onShowSuccessScreen(() => {
          showSuccessScreen();
        });
      }

      // ============================================================
      // AWAY MODE UI HANDLERS
      // ============================================================
      const awayModeIndicator = document.getElementById("away-mode-indicator");

      function setAwayModeIndicator(active) {
        if (awayModeIndicator) {
          awayModeIndicator.style.display = active ? "block" : "none";
        }
      }

      // Away Mode IPC listeners
      if (window.electronAPI) {
        window.electronAPI.onAwayModeEnabled?.(() => {
          console.log("[UI] Away Mode enabled");
          setAwayModeIndicator(true);
        });

        window.electronAPI.onAwayModeDisabled?.(() => {
          console.log("[UI] Away Mode disabled");
          setAwayModeIndicator(false);
        });

        window.electronAPI.onAwayModePreflightFailed?.((errors) => {
          console.log("[UI] Away Mode preflight failed:", errors);
          setAwayModeIndicator(false);
          // Could show error toast here
        });

        // User returned modal removed - Away Mode controlled from Dashboard

        // Camera check listener - DISABLED in v2.1.0
        // Away Mode does NOT require camera access. This listener is kept
        // for backward compatibility but will always return false without
        // actually accessing the camera hardware.
        window.electronAPI.onAwayModeCheckCamera?.(async () => {
          console.log("[UI] Camera check requested (IGNORED - camera not needed for Away Mode)");
          // IMPORTANT: Do NOT call getUserMedia here!
          // Always return false - camera check is no longer needed for Away Mode.
          window.electronAPI.sendCameraCheckResult(false);
        });

        // DEBUG: Power blocker status - will show in DevTools console
        if (typeof window.electronAPI.onPowerBlockerStatus === 'function') {
          console.log('[UI] PowerBlocker listener registered');
          window.electronAPI.onPowerBlockerStatus((data) => {
          console.log(`[UI] üîã POWER BLOCKER: ${data.status} (id: ${data.id})`);
          if (data.status === 'STOPPED') {
            console.log('[UI] ‚úÖ Power save blocker STOPPED - system CAN now sleep');
          } else if (data.status === 'STARTED') {
            console.log('[UI] ‚ö° Power save blocker ACTIVE - system will NOT sleep');
          } else if (data.status === 'ALREADY_NULL') {
            console.log('[UI] ‚ÑπÔ∏è Power save blocker was already stopped');
          }
          });
        } else {
          console.warn('[UI] ‚ö†Ô∏è onPowerBlockerStatus is MISSING on window.electronAPI (preload.js not updated / not loaded)');
        }
      }

      // User Returned Modal removed - Away Mode controlled manually from Dashboard

      // ============================================================
      // INIT
      // ============================================================
      setLanguage("en");
      document.getElementById("lang-en").addEventListener("click", () => setLanguage("en"));
      document.getElementById("lang-he").addEventListener("click", () => setLanguage("he"));
      codeInputs[0]?.focus();
    </script>
  </body>
</html>
